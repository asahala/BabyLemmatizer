#!/usr/bin/python
# -*- coding: utf-8 -*-

import json
from collections import defaultdict
from collections import Counter

""" Disambiguator for Neural BabyLemmatizer      asahala 2021-04

This script provides very naïve disambiguation for lemmata
based on their surrounding POS-tags. This fixes some of the
most obvious cases and improves logogram lemmatization accuracy
by ca. 1% as well as provides best lemmatization candidates
with highest possible confidence score to avoid unnecessary
manual work verifying the results.

"""

def readfile(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        return f.read().splitlines()

def readjson(filename):
    with open(filename) as json_file:
        return json.load(json_file)

def get_pos(fields):
    if len(fields) > 1:
        pos = fields[3]
        return pos
    else:
        return 'END'

def disambiguate(entry, lookup, threshold=0):

    """ Replace amgiguous logograms with their most common
    lemma in the context

    :param entry           a line of conllu data
    :param lookup          context lookup generated by
                           get_context()
    
    :type entry            str
    :type lookup           dict

    """
    fields = entry.split('\t')
    if len(fields) > 2:
        xlit = fields[1]
        lemma = fields[2]
        pos = fields[3]
        conf = fields[-1]

        # temp code

        if pos == 'V':
            if set(lemma) - set('īūî') != set(lemma):
                print(xlit, lemma, pos, conf, sep='\t')

        # end temp code
        
        context = fields[5]        
        if xlit in lookup.keys():
            lemmata = lookup[xlit].get(context, [(lemma, 666)])
            if lemmata[0][1] > threshold:
                lemma = lemmata[0][0]
                if xlit.islower():
                    fields[-1] = '4.0'
        fields[2] = lemma

        return '\t'.join(fields)

    return entry    

def get_context(data):
    """ Get lemma POS contexts and store them as a dict lookup

    :param data          conllu data with context added
    :type data           list

    {transliteration:
          {context: [lemmas], ...}
     ... }

    """

    cont = {}
    lemmafreqs = []

    """ Calculate lemma freqs in contexts and disregarding
    their contexts """
    for line in data:
        fields = line.split('\t')

        if len(fields) > 2:
            xlit = fields[1]
            lemma = fields[2]
            context = fields[5]
            if context:
                lemmafreqs.append(lemma)    
                if xlit not in cont.keys():
                    cont[xlit] = defaultdict(list)
                else:
                    cont[xlit][context].append(lemma)

    lemmafreqs = Counter(lemmafreqs)

    for xlit in cont:
        for context in cont[xlit]:
            """ Make tie-breakers by ranking most common lemmata (disregarding
            its contexts) and adding the reciprocal of its rank to the context-wise
            lemma counts. """
            lemmacounts = Counter(cont[xlit][context])
            most_common = sorted(((lemmafreqs[l], l) for l in lemmacounts), reverse=True)
            for i, lem in enumerate(most_common, start=1):
                lemmacounts[lem[1]] += (1 / i)
            cont[xlit][context] = sorted(lemmacounts.items(),
                                                 reverse=True,
                                                 key=lambda item: item[1])
    return cont


def add_context(data, pos_map_filename, simplify=False):
    """ Adds POS context (preceding, current, following) for each
    wordform <-> lemma pair in the Conll-u file. Use the following
    special POS tags for marking boundaries. 

        BEG        Null-tag at the start of text unit
        END        Null-tag at the end of text unit

    Yields data in conllu format with context information.

    :param data              conllu file where the contexts are
                             to be added.
    :param pos_map_filename  JSON file with Oracc : Korp POS-mapping
    :param simplify          use simplified Korp pos tags

    :type data               list
    :type pos_map_filename   str
    :type simplify           bool

    NOTE: It seems that the contexts are better captured with
    more diverse Oracc POS-tags. Thus ´simplify´ is disabled by
    default. 
    """

    """ Read Oracc-Korp mappings to simplify pos tags """
    if simplify:
        pos_dict = readjson(pos_map_filename)
    
    last_pos = 'BEG'
    for no, line in enumerate(data):
        new_segment = False
        fields = line.split('\t')

        if no+1 < len(data):
            next_pos = get_pos(data[no+1].split('\t'))
        else:
            next_pos = 'END'

        if len(fields) < 2:
            new_segment = True#last_post = 'EOL'
            yield ''
        else:
            if False:#fields[1].islower():
                """ Ignore syllabic spelling; NOTE: apparently
                it's not necessary to ignore them """
                yield '\t'.join(fields)
            else:
                this_pos = fields[3]
                if simplify:
                    """ Simplification seems to lower the
                    performance """
                    this_pos = pos_dict.get(this_pos, this_pos)
                    last_pos = pos_dict.get(last_pos, last_pos)
                    next_pos = pos_dict.get(next_pos, next_pos)
                yield '\t'.join(fields[0:5] \
                                + [last_pos + '|' + this_pos + '|' + next_pos] \
                                + fields[6:])
        if not new_segment:
            last_pos = get_pos(fields)
        else:
            last_pos = 'BEG'
            
#data = add_context(readfile('../data/akk-output.txt'), 'pos-map.json')

