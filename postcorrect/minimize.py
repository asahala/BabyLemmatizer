#!/usr/bin/python
# -*- coding: utf-8 -*-

import re
from collections import defaultdict

"""                                                      asahala 2021
                                            http://github.com/asahala
BabyLemmatizer pseudo-transcriber

This script generates deterministic pseudo-transcription for
Akkadian transliteration. The goal is to simplify the transliteration
as much as possible without adding too much ambiguity.

For example,

   DINGIR.MEŠ-su        --> DINGIR
   DINGIR-ia            --> DINGIR
   i-za-ak-ka-ar        --> izakar   
   i-za-kar             --> izakar
   iz-za-kar            --> izakar

which converge to lemmas ´ilu´ and ´zakāru´ respectively. The general
rules are:

  1. Get rid of all non-alphabetic characters except those that
     are used to mark determinatives or phonetic complements,
     or if they are subscript indices for logograms.
  2. Merge all double vowels and consonants.
  3. Strip pronominal suffixes from logograms.

NOTE! The process is not reversable without saving every instance
of transliteration -> transcription, thus make sure you save the
´VOCABULARY_FILE´ generated by this script and never modify the text
or word order in the conllu files!

With all the necessary files, this process can be reversed with
´fill_unambgiuous.py´. This should be done after the corpus
has been lemmatized and POS-tagged with the neural parser.

"""

""" Regex for most common pronominal suffixes """
morphemes = re.compile(r'((-ia₂?)|(-k[ai](-ma)?)|(-ku-nu(-ma)?)|(-ši(-in)?'\
            '(-na)?(-ma)?)|(-[šs]u₂?)(-un)?(-nu)?(-ti)?(-ma)?)$')

def read_file(filename):
    """ General conllu reader """
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f.read().splitlines():
            if line:
                yield line.split('\t')
            else:
                yield
    
def strip_logograms(xlit):
    """ Handle logograms and logo-syllabic writing """
    # Kill plural marker and subsequent morphemes
    # xlit = re.sub('[-\.]MEŠ.+', '', xlit)

    # Kill pronominal suffixes from logo-syllabic non-proper nouns.
    # (this appears to be beneficial for pos-tagging but bad for
    # lemmatization
    #if not xlit.isupper() and not xlit.islower():
    #    if not re.match('.*\{[mdf]\}.*', xlit):
    #        xlit = re.sub(morphemes, '', xlit)
    return xlit
    
def _minimize(xlit):
    """ General pseudo-transcriber. """
    pre = ''
    for c in xlit:
        if (pre != c and c.isalpha()) or c in ('}', '{'):
            yield c
        elif pre.isupper() and c.isdigit():
            yield c
            
        if c.isalpha():
            pre = c

def minimize_conllu(filename):
    """ Minimizes transliterations and returns minimized
    connlu and a list of original transliterations that
    can be used later to reverse the minimization. """
    vocab_minimized = set()
    original = []
    minimized = []
    lemmas = []
    for fields in read_file(filename):
        if fields is not None:
            xlit = strip_logograms(fields[1])
            xlit = ''.join(_minimize(xlit))
            original.append(fields[1])
            lemmas.append(fields[2])
            if xlit:
                fields[1] = xlit
            minimized.append('\t'.join(fields))
            vocab_minimized.add(fields[1])   
        else:
            minimized.append('')
            original.append('')
            lemmas.append('#')

    print('> vocabulary of {} reduced from {} -> {}'.format(filename,
                                               len(set(original)),
                                               len(vocab_minimized)))
    return (minimized, original, lemmas)

def write_conllu(filename, data):
    with open(filename, 'w', encoding='utf-8') as f:
        for line in data:
            f.write(line + '\n')
    print('> Wrote ' + filename)

def count_oov(train_file, test_file):
    """ Count the number of OOV in train and test files.
    Can be used to test how much the OOV rate changes. Gives
    also average ambiguity (i.e. how many different lemma+pos
    tags transliteration has on average) """
    def get_vocab(filename, pos_lemma=False):
        for fields in read_file(filename):
            if fields is not None:
                if pos_lemma:
                    yield (fields[1], fields[2], fields[3])
                else:
                    yield fields[1]

    train_vocab = get_vocab(train_file)
    test_vocab = list(get_vocab(test_file))

    tokens = len(test_vocab)
    types = len(set(test_vocab))

    oov_types = set(test_vocab) - set(train_vocab)
    oov_tokens = sum(1 for w in test_vocab if w in oov_types)
    
    print('', '-'*62, test_file, '-'*62, sep='\n')
    print('> OOV count: types %i, tokens: %i.'\
          % (len(oov_types), oov_tokens))
    
    print('> OOV rate: types %.3f, tokens: %.3f.'\
          % (len(oov_types)/types, oov_tokens/tokens))

    for file in [train_file, test_file]:            
        ambiguity = defaultdict(set)
        for xlit, lemma, pos in get_vocab(file, pos_lemma=True):
            ambiguity[xlit].add(lemma+pos)

        print('', '-'*62, file, '-'*62, sep='\n')
        print('> Avg. ambiguity: %.3f' %\
              (sum(len(v) for v in ambiguity.values()) / len(ambiguity)))

def process(prefix):
    minimized, original, lemmas = minimize_conllu(prefix + '-test.conllu')
    write_conllu(prefix + '-minimized-test.conllu', minimized)
    write_conllu(prefix + '.xlit', original)
    write_conllu(prefix + '.lemma', lemmas)

    minimized, original, lemmas = minimize_conllu(prefix + '-train.conllu')
    write_conllu(prefix + '-minimized-train.conllu', minimized)

    minimized, original, lemmas = minimize_conllu(prefix + '-dev.conllu')
    write_conllu(prefix + '-minimized-dev.conllu', minimized)


#count_oov('babylonian-transl-mini3-train.conllu', 'babylonian-transl-mini3-test.conllu')
#count_oov('baby-minimized-train.conllu', 'baby-minimized-test.conllu')
'''

minimized, original = minimize_conllu('baby-test.conllu')
write_conllu('baby-mini-test.conllu', minimized)
write_conllu(VOCABULARY_FILE, original)

minimized, original = minimize_conllu('baby-train.conllu')
write_conllu('baby-mini-train.conllu', minimized)

minimized, original = minimize_conllu('baby-dev.conllu')
write_conllu('baby-mini-dev.conllu', minimized)
'''
