from collections import Counter
from collections import defaultdict

""" Replaces quasi-transcriptions with real transliterations
generated by ´minimize.py´.

1. Make dictionary of unambiguous lemmata from unaltered
   training data (conllu format) with get_unamb()

2. Fill quasi-transcribed conllu with transliterations
   and unambigous lemmata with fill_unamb()

3. Write filled output into file using write_conllu()


"""

#IN_VOCAB = defaultdict(set)

def read_conllu(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f.read().splitlines():
            if line:
                yield line.split('\t')
            else:
                yield

def write_conllu(filename, data):
    with open(filename, 'w', encoding='utf-8') as f:
        for l in data:
            f.write(l + '\n')

def get_unamb(filename):
    """ Get unambiguous lemmas from training data """
    unambiguous = defaultdict(set)
    for fields in read_conllu(filename):
        if fields is not None:
            xlit = fields[1]
            lemma = fields[2]
            pos = fields[3]
            unambiguous[xlit+'|'+pos].add(lemma)
    return {k:tuple(v)[0] for k,v in unambiguous.items() if len(v) == 1}

def get_unamb2(filename, threshold=0.20):
    """ Get unambiguous lemmas from training data (cut lemmas
    if they are rarer than the threshold """
    lemmadict = defaultdict(dict)
    unambiguous = defaultdict(str)
    for fields in read_conllu(filename):
        if fields is not None:
            xlit = fields[1]
            lemma = fields[2]
            pos = fields[3]
            """ UNCOMMENT TO REVERT """
            lemmadict[xlit+'|'+pos].setdefault(lemma, 0)
            lemmadict[xlit+'|'+pos][lemma] += 1

            #lemmadict[xlit].setdefault(lemma+'|'+pos, 0)
            #lemmadict[xlit][lemma+'|'+pos] += 1

            #IN_VOCAB[xlit].add((lemma, pos))
    
    for xlit, lemmata in lemmadict.items():
        total = sum(lemmata.values())

        best = [lemma for lemma, count in lemmata.items()\
                if count/total >= threshold]

        if len(best) == 1:
            unambiguous[xlit] = best[0]
        else:
            pass#print(lemmata)

        if 'KUR-KUR' in xlit:
            print(best, lemmata)
            
    return unambiguous, lemmadict


def fill_unamb(filename, xlit_filename, unambiguous, lemmadict, override=True):
    """ Fill target conllu file with unambiguous lemmata

    :param filename             output conllu from neural parser
    :param xlit_filename        transliteration file produced by
                                ´minimize.py´ """
    
    xlits = read_conllu(xlit_filename)
    predicted = read_conllu(filename)

    #for k in unambiguous:
    #    if 'TIN-TIR' in k:
    #        print(k)

    no_pos = {x.split('|')[0]: x for x in unambiguous.keys()}

    for xlit, pred in zip(xlits, predicted):

        if not override:
            if xlit is None:
                yield ''
            else:
                yield '\t'.join(pred + ['---'])
            continue

        if xlit is None:
            yield ''
        else:
            #yield '\t'.join(pred + ['x'])
            confidence = '1.0'
            """ Replace minimized transliteration with real
            transliteration """
            pred[1] = xlit[0]
            key = pred[1] + '|' + pred[3]
            #key = pred[1]# + '|' + pred[3]

            correct_lemma = unambiguous.get(key, None)
            if correct_lemma is not None:
                #if pred[2] != correct_lemma:
                #    print(pred[2], '->', correct_lemma, pred[1], sep='\t')
                pred[2] = correct_lemma
                #print(key)
                confidence = '3.0'
            else:
                ppos, plem = pred[3], pred[2]
                tgt = lemmadict.get(xlit[0] + '|' + ppos)
                if tgt is None:
                    pass
                elif plem in tgt:
                    confidence = '2.0'
                
            """ Force PN pos tag for proper names """
            #if xlit[0].startswith(('{f}', '{m}')) and 'x' not in xlit[0]:
            #    if pred[3] != 'PN':
            #        pred[3] = 'PN'
            #elif xlit[0].startswith('{d}') and 'x' not in xlit[0]:
            #    if pred[3] != 'DN':
            #        pred[3] = 'DN'
                
            """ Check if neural net has made strange lemmata for in-vocab
            words """
            possible_lemmata = lemmadict.get(key, None)
            if possible_lemmata is not None:
                if pred[2] not in possible_lemmata.keys():
                    #best = max(possible_lemmata, key=possible_lemmata.get)
                    #print(pred[2], '->', best)
                    #print(pred[2], 'oli')
                    pred[2] = max(possible_lemmata, key=possible_lemmata.get)
                    confidence = '3.0'
                    #print(pred[2], 'on nyt')
            else:
                #print(pred)
                """ TODO: Do same for OOV words but more carefully: e.g. check
                if the predictions look impossible or run quasi-transcription
                for them to get matches """
                pass

            """ Debug POS-tags; Attempt to fix clearly incorrect pos tags and lemmas
            """
            ppred, ppos = pred[2], pred[3]

            amp = '&amp;'
            if amp in ppred and amp not in ppos:
                pred[2] = ppred.split(amp)[0]
            elif amp not in ppred and amp in ppos:
                pred[3] = ppos.split(amp)[0]
            
            
            yield '\t'.join(pred + [confidence])
